{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68630456",
   "metadata": {},
   "source": [
    "# ABCDE Segmentation of spend\n",
    "\n",
    "This notebook performs a simple ABCDE-segmentation of spend data, and exports results to .xlsx files.\n",
    "Run each cell in the order they appear.\n",
    "\n",
    "Scope (class2/brand) can be adjusted by changing the query. If changed, check compability for export function (exported columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc14b48",
   "metadata": {},
   "source": [
    "### Imports, data loading & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac70552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "import source.data_processing.export_utils as eu\n",
    "eu = importlib.reload(eu)\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Make attached helper modules importable (these are present next to the notebook or in /mnt/data)\n",
    "EXTRA_MODULE_DIRS = [\n",
    "    \".\", \"/mnt/data\",  # adjust if your helpers live elsewhere\n",
    "]\n",
    "for p in EXTRA_MODULE_DIRS:\n",
    "    if p not in sys.path:\n",
    "        sys.path.insert(0, p)\n",
    "\n",
    "# Import our helper utilities (from your attachments)\n",
    "from source.data_prep import field_desc_utils as fdesc\n",
    "from source.data_prep import field_value_utils as fval\n",
    "from source.data_processing import analysis_utils as au\n",
    "from source.data_processing.analysis_utils import compute_abcde_per_class4\n",
    "from source.data_processing.export_utils import export_year_split_purchase_quantity\n",
    "from source.data_processing import export_utils as xpu\n",
    "import sql_queries as qreg\n",
    "from source.db_connect import bigquery_connector\n",
    "\n",
    "load_dotenv()  # reads .env in the working directory\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "DATASET_ID = os.getenv(\"DATASET_ID\")\n",
    "TABLE_ID   = os.getenv(\"TABLE_ID\")\n",
    "OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\", \"./output\")\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(\"PROJECT_ID:\", PROJECT_ID, \"| DATASET_ID:\", DATASET_ID, \"| TABLE_ID:\", TABLE_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2b904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build fully qualified table and fetch data\n",
    "fqtn = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
    "print(\"Reading from:\", fqtn)\n",
    "\n",
    "# Use correct import for BigQueryConnector\n",
    "from source.db_connect import bigquery_connector\n",
    "bq = bigquery_connector.BigQueryConnector(project_id=PROJECT_ID)  # uses default creds or GOOGLE_APPLICATION_CREDENTIALS\n",
    "# Use analysis_utils registry to fetch the table you described\n",
    "df = au.fetch_purchase_data(bq_client=bq)\n",
    "print(df.shape, \"rows x cols\")\n",
    "df.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78fac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print columns for debugging after preprocessing\n",
    "# Ensure df is defined before preprocessing!\n",
    "try:\n",
    "    df\n",
    "except NameError:\n",
    "    # If df is not defined, load it first\n",
    "    fqtn = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
    "    print(\"Reading from:\", fqtn)\n",
    "    bq = bigquery_connector.BigQueryConnector(project_id=PROJECT_ID)\n",
    "    df = au.fetch_purchase_data(bq_client=bq)\n",
    "    print(df.shape, \"rows x cols\")\n",
    "    df.head(3)\n",
    "\n",
    "df = au.preprocess_detailed_data(df)\n",
    "print(\"Columns in DataFrame after preprocessing:\", list(df.columns))\n",
    "\n",
    "# Ensure key columns exist with consistent names\n",
    "def _resolve(df, candidates):\n",
    "    # mirror analysis_utils._resolve_col\n",
    "    def norm(s): return re.sub(r\"[^a-z0-9]\", \"\", s.lower())\n",
    "    by_norm = {norm(c): c for c in df.columns}\n",
    "    for c in candidates:\n",
    "        if c in df.columns: return c\n",
    "        if norm(c) in by_norm: return by_norm[norm(c)]\n",
    "    return candidates[0]  # fallback to first candidate, but user must check resolved value\n",
    "\n",
    "# Force correct column names for robustness\n",
    "COL_CLASS4 = \"Class4\"\n",
    "COL_PNUM   = _resolve(df, [\"ProductNumber\"])\n",
    "COL_PDESC  = _resolve(df, [\"ProductDescription\"])\n",
    "COL_EUR    = _resolve(df, [\"Amount Eur\", \"Purchase Amount Eur\", \"amount_eur\", \"purchase_amount_eur\"])\n",
    "COL_QTY    = _resolve(df, [\"Purchase Quantity\", \"Quantity\"])\n",
    "print(\"Resolved:\", {\"Class4\":COL_CLASS4, \"ProductNumber\":COL_PNUM, \"ProductDescription\":COL_PDESC, \"â‚¬\":COL_EUR, \"Qty\":COL_QTY})\n",
    "\n",
    "# Normalize spend to numeric\n",
    "df[COL_EUR] = pd.to_numeric(df[COL_EUR], errors=\"coerce\").fillna(0.0)\n",
    "df[COL_QTY] = pd.to_numeric(df[COL_QTY], errors=\"coerce\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78128afd",
   "metadata": {},
   "source": [
    "### ABCDE Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b6bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the correct DataFrame name (df) for aggregation\n",
    "product_spend_df = (\n",
    "    df\n",
    "        .groupby([\"Class4\", \"ProductNumber\", \"ProductDescription\"], as_index=False)\n",
    "        .agg({\"Purchase Amount Eur\": \"sum\"})\n",
    ")\n",
    "print(product_spend_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311d236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented = compute_abcde_per_class4(\n",
    "    df=product_spend_df,\n",
    "    class_col=\"Class4\",\n",
    "    product_col=\"ProductNumber\",      # unused in function, but fine\n",
    "    spend_col=\"Purchase Amount Eur\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b09a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the correct DataFrame name (df) for merging segmentation results\n",
    "purchase_with_seg = df.merge(\n",
    "    segmented[[\"ProductNumber\", \"Segmentation\"]],\n",
    "    on=\"ProductNumber\",\n",
    "    how=\"left\",\n",
    ")\n",
    "print(purchase_with_seg.head())\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c911d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize tiers by Class4 (robust, no EUR mismatch, dedup tags)\n",
    "# 1. Aggregate EUR per product in original df\n",
    "# 2. Merge segmentation tags (deduplicated) onto this per-product EUR DataFrame\n",
    "# 3. Use this for summary\n",
    "\n",
    "# Aggregate EUR per product\n",
    "per_product = df.groupby([COL_CLASS4, COL_PNUM], as_index=False)[COL_EUR].sum()\n",
    "# Compute and deduplicate segmentation tags\n",
    "seg_tags = compute_abcde_per_class4(df, COL_CLASS4, COL_PNUM, COL_EUR)\n",
    "seg_tags = seg_tags.drop_duplicates(subset=[COL_CLASS4, COL_PNUM])\n",
    "# Merge segmentation tags\n",
    "per_product = per_product.merge(\n",
    "    seg_tags,\n",
    "    on=[COL_CLASS4, COL_PNUM], how=\"left\"\n",
    ")\n",
    "\n",
    "# Print columns for debugging\n",
    "print(\"Columns in per_product:\", list(per_product.columns))\n",
    "\n",
    "# Find EUR column by substring match (case-insensitive)\n",
    "eur_cols = [col for col in per_product.columns if 'eur' in col.lower()]\n",
    "print(\"Columns containing 'eur':\", eur_cols)\n",
    "if eur_cols:\n",
    "    # Prefer the first column that ends with '_x', else just the first\n",
    "    eur_col_candidates = [col for col in eur_cols if col.endswith('_x')]\n",
    "    actual_eur_col = eur_col_candidates[0] if eur_col_candidates else eur_cols[0]\n",
    "    print(f\"Using EUR column for aggregation: {actual_eur_col}\")\n",
    "else:\n",
    "    raise KeyError(f\"No column containing 'eur' found in per_product. Columns: {list(per_product.columns)}\")\n",
    "\n",
    "# Summary by Class4 and Segmentation\n",
    "summary = (\n",
    "    per_product.groupby([COL_CLASS4, \"Segmentation\"], as_index=False)\n",
    "    .agg(\n",
    "        products=(COL_PNUM, \"nunique\"),\n",
    "        purchase_amount_eur_total=(actual_eur_col, \"sum\")\n",
    "    )\n",
    "    .sort_values([COL_CLASS4, \"Segmentation\"])\n",
    ")\n",
    "# Format # products as e.g. 1,000\n",
    "summary[\"products_fmt\"] = summary[\"products\"].apply(lambda x: f\"{x:,}\")\n",
    "# Format purchase_amount_eur_total as e.g. 1,000,000 EUR\n",
    "summary[\"purchase_amount_eur_total_fmt\"] = summary[\"purchase_amount_eur_total\"].apply(lambda x: f\"{x:,.0f} EUR\")\n",
    "summary[[COL_CLASS4, \"Segmentation\", \"products_fmt\", \"purchase_amount_eur_total_fmt\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71989d29",
   "metadata": {},
   "source": [
    "### Data validation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2d41d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the raw total from the original table (no deduplication)\n",
    "raw_total = df[COL_EUR].sum()\n",
    "print(f\"Raw total from original table: {raw_total:,.0f} EUR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce899f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the sum of purchase_amount_eur_total in summary matches the raw total from the original table\n",
    "total_summary = summary[\"purchase_amount_eur_total\"].sum()\n",
    "print(f\"Summary tiers by Class4 total: {total_summary:,.0f} EUR\")\n",
    "if np.isclose(total_summary, raw_total):\n",
    "    print(\"Summary tiers by Class4 total matches raw total from original table.\")\n",
    "else:\n",
    "    print(\"Summary tiers by Class4 total does NOT match raw total from original table!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c42d30",
   "metadata": {},
   "source": [
    "### Export results to .xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc44d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import source.data_processing.export_utils as eu\n",
    "eu = importlib.reload(eu)\n",
    "\n",
    "from source.data_processing.analysis_utils import compute_abcde_per_class4\n",
    "from source.data_processing.export_utils import export_year_split_purchase_quantity, fetch_year_purchase_quantity\n",
    "\n",
    "\n",
    "# # 1) Compute per-product spend for ABCDE from your preprocessed df\n",
    "product_spend_df = (\n",
    "    df\n",
    "      .groupby([\"Class4\", \"ProductNumber\", \"ProductDescription\"], as_index=False)\n",
    "      .agg({\"Purchase Amount Eur\": \"sum\"})\n",
    ")\n",
    "\n",
    "segmented = compute_abcde_per_class4(\n",
    "    df=product_spend_df,\n",
    "    class_col=\"Class4\",\n",
    "    product_col=\"ProductNumber\",\n",
    "    spend_col=\"Purchase Amount Eur\",\n",
    ")\n",
    "\n",
    "# 2) Merge Segmentation back into your enriched df\n",
    "purchase_with_seg = df.merge(\n",
    "    segmented[[\"ProductNumber\", \"Segmentation\"]],\n",
    "    on=\"ProductNumber\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# 3) Build segmentation_df with only columns that exist in the DataFrame, now including Class4 and ProductDescription\n",
    "pretty_cols = [\n",
    "    \"Class4\",\n",
    "    \"ProductNumber\",\n",
    "    \"ProductDescription\",\n",
    "    \"Segmentation\",\n",
    "    \"SalesRounding\",\n",
    "]\n",
    "existing_cols = [col for col in pretty_cols if col in purchase_with_seg.columns]\n",
    "segmentation_df = purchase_with_seg[existing_cols].drop_duplicates(\"ProductNumber\")\n",
    "\n",
    "print(\"segmentation_df:\", segmentation_df.shape, segmentation_df.columns.tolist())\n",
    "\n",
    "# (Optional) sanity check: df_year\n",
    "df_year = fetch_year_purchase_quantity(\n",
    "    bq,\n",
    "    table=\"kramp-sharedmasterdata-prd.MadsH.purchase_data\",\n",
    ")\n",
    "print(\"df_year:\", df_year.shape, df_year.columns.tolist())\n",
    "\n",
    "# Standardize class4 column name in df_year to match segmentation_df\n",
    "if 'class4' in df_year.columns:\n",
    "    df_year = df_year.rename(columns={'class4': 'Class4'})\n",
    "\n",
    "# Ensure ProductDescription is present in df_year_merged after merge\n",
    "if 'ProductDescription' not in df_year.columns and 'ProductDescription' in segmentation_df.columns:\n",
    "    df_year['ProductDescription'] = df_year['ProductNumber'].map(\n",
    "        dict(zip(segmentation_df['ProductNumber'], segmentation_df['ProductDescription']))\n",
    "    )\n",
    "\n",
    "# 4) Merge segmentation_df into df_year on ProductNumber only (to avoid Class4 mismatch)\n",
    "df_year_merged = df_year.merge(\n",
    "    segmentation_df,\n",
    "    on=[\"ProductNumber\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_seg\")\n",
    ")\n",
    "\n",
    "# If ProductDescription is missing after merge, fill from segmentation_df\n",
    "if 'ProductDescription' not in df_year_merged.columns and 'ProductDescription_seg' in df_year_merged.columns:\n",
    "    df_year_merged['ProductDescription'] = df_year_merged['ProductDescription_seg']\n",
    "\n",
    "# 5) Call the export utility with the merged DataFrame, including Class4 and ProductDescription if present\n",
    "written_files = export_year_split_purchase_quantity(\n",
    "    bq,\n",
    "    output_dir=\"./output\",\n",
    "    table=\"kramp-sharedmasterdata-prd.MadsH.purchase_data\",\n",
    "    fmt_thousands=True,\n",
    "    merged_header_label=\"PurchaseQuantity\",\n",
    "    segmentation_df=df_year_merged,\n",
    "    segmentation_col=\"Segmentation\",\n",
    ")\n",
    "\n",
    "print(\"\\nWritten Excel files:\")\n",
    "for f in written_files:\n",
    "    print(\" -\", f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
