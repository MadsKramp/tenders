{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68630456",
   "metadata": {},
   "source": [
    "# ABCDE Segmentation of spend\n",
    "\n",
    "This notebook performs a simple ABCDE-segmentation of spend data, and exports results to .xlsx files.\n",
    "Run each cell in the order they appear.\n",
    "\n",
    "Scope (class2/brand) can be adjusted by changing the query. If changed, check compability for export function (exported columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc14b48",
   "metadata": {},
   "source": [
    "### Imports, data loading & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac70552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "import source.data_processing.export_utils as eu\n",
    "eu = importlib.reload(eu)\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Make attached helper modules importable (these are present next to the notebook or in /mnt/data)\n",
    "EXTRA_MODULE_DIRS = [\n",
    "    \".\", \"/mnt/data\",  # adjust if your helpers live elsewhere\n",
    "]\n",
    "for p in EXTRA_MODULE_DIRS:\n",
    "    if p not in sys.path:\n",
    "        sys.path.insert(0, p)\n",
    "\n",
    "# Import our helper utilities (from your attachments)\n",
    "from source.data_prep import field_desc_utils as fdesc\n",
    "from source.data_prep import field_value_utils as fval\n",
    "from source.data_processing import analysis_utils as au\n",
    "from source.data_processing.analysis_utils import compute_abcde_per_class4\n",
    "from source.data_processing.export_utils import export_year_split_purchase_quantity\n",
    "from source.data_processing import export_utils as xpu\n",
    "import sql_queries as qreg\n",
    "from source.db_connect import bigquery_connector\n",
    "\n",
    "load_dotenv()  # reads .env in the working directory\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "DATASET_ID = os.getenv(\"DATASET_ID\")\n",
    "TABLE_ID   = os.getenv(\"TABLE_ID\")\n",
    "OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\", \"./output\")\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(\"PROJECT_ID:\", PROJECT_ID, \"| DATASET_ID:\", DATASET_ID, \"| TABLE_ID:\", TABLE_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2b904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build fully qualified table and fetch data\n",
    "fqtn = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
    "print(\"Reading from:\", fqtn)\n",
    "\n",
    "# Use correct import for BigQueryConnector\n",
    "from source.db_connect import bigquery_connector\n",
    "bq = bigquery_connector.BigQueryConnector(project_id=PROJECT_ID)  # uses default creds or GOOGLE_APPLICATION_CREDENTIALS\n",
    "# Use analysis_utils registry to fetch the table you described\n",
    "df = au.fetch_purchase_data(bq_client=bq)\n",
    "print(df.shape, \"rows x cols\")\n",
    "df.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442652c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize column names to match notebook expectations\n",
    "# BigQuery returns lowercase column names, but the notebook uses mixed case\n",
    "column_mapping = {\n",
    "    'class4': 'Class4',\n",
    "    'purchase_amount_eur': 'Purchase Amount Eur',\n",
    "    'purchase_quantity': 'Purchase Quantity'\n",
    "}\n",
    "\n",
    "df = df.rename(columns=column_mapping)\n",
    "print(f\"✓ Standardized column names\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78128afd",
   "metadata": {},
   "source": [
    "### ABCDE Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b6bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the correct DataFrame name (df) for aggregation\n",
    "product_spend_df = (\n",
    "    df\n",
    "        .groupby([\"Class4\", \"ProductNumber\", \"ProductDescription\"], as_index=False)\n",
    "        .agg({\"Purchase Amount Eur\": \"sum\"})\n",
    ")\n",
    "print(product_spend_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311d236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented = compute_abcde_per_class4(\n",
    "    df=product_spend_df,\n",
    "    class_col=\"Class4\",\n",
    "    product_col=\"ProductNumber\",      # unused in function, but fine\n",
    "    spend_col=\"Purchase Amount Eur\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b09a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the correct DataFrame name (df) for merging segmentation results\n",
    "purchase_with_seg = df.merge(\n",
    "    segmented[[\"ProductNumber\", \"Segmentation\"]],\n",
    "    on=\"ProductNumber\",\n",
    "    how=\"left\",\n",
    ")\n",
    "print(purchase_with_seg.head())\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c911d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize tiers by Class4\n",
    "# Use the already computed segmentation from segmented DataFrame\n",
    "summary = (\n",
    "    segmented\n",
    "    .groupby([\"Class4\", \"Segmentation\"], as_index=False)\n",
    "    .agg(\n",
    "        products=(\"ProductNumber\", \"nunique\"),\n",
    "        purchase_amount_eur_total=(\"Purchase Amount Eur\", \"sum\")\n",
    "    )\n",
    "    .sort_values([\"Class4\", \"Segmentation\"])\n",
    ")\n",
    "\n",
    "# Format # products as e.g. 1,000\n",
    "summary[\"products_fmt\"] = summary[\"products\"].apply(lambda x: f\"{x:,}\")\n",
    "# Format purchase_amount_eur_total as e.g. 1,000,000 EUR\n",
    "summary[\"purchase_amount_eur_total_fmt\"] = summary[\"purchase_amount_eur_total\"].apply(lambda x: f\"{x:,.0f} EUR\")\n",
    "\n",
    "print(\"Summary by Class4 and Segmentation:\")\n",
    "summary[[\"Class4\", \"Segmentation\", \"products_fmt\", \"purchase_amount_eur_total_fmt\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71989d29",
   "metadata": {},
   "source": [
    "### Data validation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2d41d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the raw total from the original table (no deduplication)\n",
    "raw_total = df[\"Purchase Amount Eur\"].sum()\n",
    "print(f\"Raw total from original table: {raw_total:,.0f} EUR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce899f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the sum of purchase_amount_eur_total in summary matches the raw total from the original table\n",
    "total_summary = float(summary[\"purchase_amount_eur_total\"].sum())\n",
    "raw_total_float = float(raw_total)\n",
    "print(f\"Summary tiers by Class4 total: {total_summary:,.0f} EUR\")\n",
    "if np.isclose(total_summary, raw_total_float):\n",
    "    print(\"✓ Summary tiers by Class4 total matches raw total from original table.\")\n",
    "else:\n",
    "    print(\"✗ Summary tiers by Class4 total does NOT match raw total from original table!\")\n",
    "    print(f\"  Difference: {total_summary - raw_total_float:,.0f} EUR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c42d30",
   "metadata": {},
   "source": [
    "### Export results to .xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc44d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import source.data_processing.export_utils as eu\n",
    "eu = importlib.reload(eu)\n",
    "\n",
    "from source.data_processing.analysis_utils import compute_abcde_per_class4\n",
    "from source.data_processing.export_utils import export_year_split_purchase_quantity, fetch_year_purchase_quantity\n",
    "\n",
    "\n",
    "# # 1) Compute per-product spend for ABCDE from your preprocessed df\n",
    "product_spend_df = (\n",
    "    df\n",
    "      .groupby([\"Class4\", \"ProductNumber\", \"ProductDescription\"], as_index=False)\n",
    "      .agg({\"Purchase Amount Eur\": \"sum\"})\n",
    ")\n",
    "\n",
    "segmented = compute_abcde_per_class4(\n",
    "    df=product_spend_df,\n",
    "    class_col=\"Class4\",\n",
    "    product_col=\"ProductNumber\",\n",
    "    spend_col=\"Purchase Amount Eur\",\n",
    ")\n",
    "\n",
    "# 2) Merge Segmentation back into your enriched df\n",
    "purchase_with_seg = df.merge(\n",
    "    segmented[[\"ProductNumber\", \"Segmentation\"]],\n",
    "    on=\"ProductNumber\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# 3) Build segmentation_df with only columns that exist in the DataFrame, now including Class4 and ProductDescription\n",
    "pretty_cols = [\n",
    "    \"Class4\",\n",
    "    \"ProductNumber\",\n",
    "    \"ProductDescription\",\n",
    "    \"Segmentation\",\n",
    "    \"SalesRounding\",\n",
    "]\n",
    "existing_cols = [col for col in pretty_cols if col in purchase_with_seg.columns]\n",
    "segmentation_df = purchase_with_seg[existing_cols].drop_duplicates(\"ProductNumber\")\n",
    "\n",
    "print(\"segmentation_df:\", segmentation_df.shape, segmentation_df.columns.tolist())\n",
    "\n",
    "# (Optional) sanity check: df_year\n",
    "df_year = fetch_year_purchase_quantity(\n",
    "    bq,\n",
    "    table=\"kramp-sharedmasterdata-prd.MadsH.purchase_data\",\n",
    ")\n",
    "print(\"df_year:\", df_year.shape, df_year.columns.tolist())\n",
    "\n",
    "# Standardize class4 column name in df_year to match segmentation_df\n",
    "if 'class4' in df_year.columns:\n",
    "    df_year = df_year.rename(columns={'class4': 'Class4'})\n",
    "\n",
    "# Ensure ProductDescription is present in df_year_merged after merge\n",
    "if 'ProductDescription' not in df_year.columns and 'ProductDescription' in segmentation_df.columns:\n",
    "    df_year['ProductDescription'] = df_year['ProductNumber'].map(\n",
    "        dict(zip(segmentation_df['ProductNumber'], segmentation_df['ProductDescription']))\n",
    "    )\n",
    "\n",
    "# 4) Merge segmentation_df into df_year on ProductNumber only (to avoid Class4 mismatch)\n",
    "df_year_merged = df_year.merge(\n",
    "    segmentation_df,\n",
    "    on=[\"ProductNumber\"],\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_seg\")\n",
    ")\n",
    "\n",
    "# If ProductDescription is missing after merge, fill from segmentation_df\n",
    "if 'ProductDescription' not in df_year_merged.columns and 'ProductDescription_seg' in df_year_merged.columns:\n",
    "    df_year_merged['ProductDescription'] = df_year_merged['ProductDescription_seg']\n",
    "\n",
    "# 5) Call the export utility with the merged DataFrame, including Class4 and ProductDescription if present\n",
    "written_files = export_year_split_purchase_quantity(\n",
    "    bq,\n",
    "    output_dir=\"./output\",\n",
    "    table=\"kramp-sharedmasterdata-prd.MadsH.purchase_data\",\n",
    "    fmt_thousands=True,\n",
    "    merged_header_label=\"PurchaseQuantity\",\n",
    "    segmentation_df=df_year_merged,\n",
    "    segmentation_col=\"Segmentation\",\n",
    ")\n",
    "\n",
    "print(\"\\nWritten Excel files:\")\n",
    "for f in written_files:\n",
    "    print(\" -\", f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431022b6",
   "metadata": {},
   "source": [
    "### Optional function: Merge all exports into single sheeted .xlsx file ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7860a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all exported .xlsx files into a single Excel file with one sheet (vertical append)\n",
    "# Uses existing variables: written_files, OUTPUT_DIR, pd, Path\n",
    "\n",
    "if not isinstance(written_files, list) or len(written_files) == 0:\n",
    "    print(\"No exported files found in 'written_files'. Nothing to merge.\")\n",
    "else:\n",
    "    target = Path(OUTPUT_DIR) / \"ABC_Segmentation_ALL_years_combined.xlsx\"\n",
    "    combined = []\n",
    "    total_sheets = 0\n",
    "\n",
    "    for fpath in written_files:\n",
    "        try:\n",
    "            xls = pd.ExcelFile(fpath)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {fpath}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for sheet in xls.sheet_names:\n",
    "            try:\n",
    "                df_ = xls.parse(sheet_name=sheet)\n",
    "            except Exception as e:\n",
    "                print(f\"  Failed reading {sheet} from {fpath}: {e}\")\n",
    "                continue\n",
    "            # Track origin to keep context after merging\n",
    "            df_[\"__source_file\"] = Path(fpath).name\n",
    "            df_[\"__source_sheet\"] = sheet\n",
    "            combined.append(df_)\n",
    "            total_sheets += 1\n",
    "\n",
    "    if not combined:\n",
    "        print(\"No data read from any sheets. Nothing to write.\")\n",
    "    else:\n",
    "        combined_df = pd.concat(combined, ignore_index=True, sort=False)\n",
    "        with pd.ExcelWriter(target, engine=\"openpyxl\") as writer:\n",
    "            combined_df.to_excel(writer, index=False, sheet_name=\"AllData\")\n",
    "        print(f\"Merged {len(written_files)} files and {total_sheets} sheets into 1 sheet -> {target}\")\n",
    "        print(f\"Total rows: {len(combined_df):,} | Total columns: {len(combined_df.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
