{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68630456",
   "metadata": {},
   "source": [
    "# ABCDE Segmentation of spend\n",
    "\n",
    "This notebook performs a simple ABCDE-segmentation of spend data, and exports results to .xlsx files.\n",
    "Run each cell in the order they appear.\n",
    "\n",
    "Scope (class2/brand) can be adjusted by changing the query. If changed, check compability for export function (exported columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc14b48",
   "metadata": {},
   "source": [
    "### Imports, data loading & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac70552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Make attached helper modules importable (these are present next to the notebook or in /mnt/data)\n",
    "EXTRA_MODULE_DIRS = [\n",
    "    \".\", \"/mnt/data\",  # adjust if your helpers live elsewhere\n",
    "]\n",
    "for p in EXTRA_MODULE_DIRS:\n",
    "    if p not in sys.path:\n",
    "        sys.path.insert(0, p)\n",
    "\n",
    "# Import our helper utilities (from your attachments)\n",
    "from source.data_prep import field_desc_utils as fdesc\n",
    "from source.data_prep import field_value_utils as fval\n",
    "from source.data_processing import analysis_utils as au\n",
    "from source.data_processing import export_utils as xpu\n",
    "import sql_queries as qreg\n",
    "from source.db_connect import bigquery_connector\n",
    "\n",
    "load_dotenv()  # reads .env in the working directory\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "DATASET_ID = os.getenv(\"DATASET_ID\")\n",
    "TABLE_ID   = os.getenv(\"TABLE_ID\")\n",
    "OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\", \"./output\")\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(\"PROJECT_ID:\", PROJECT_ID, \"| DATASET_ID:\", DATASET_ID, \"| TABLE_ID:\", TABLE_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2b904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build fully qualified table and fetch data\n",
    "fqtn = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
    "print(\"Reading from:\", fqtn)\n",
    "\n",
    "# Use correct import for BigQueryConnector\n",
    "from source.db_connect import bigquery_connector\n",
    "bq = bigquery_connector.BigQueryConnector(project_id=PROJECT_ID)  # uses default creds or GOOGLE_APPLICATION_CREDENTIALS\n",
    "# Use analysis_utils registry to fetch the table you described\n",
    "df = au.fetch_purchase_data_enriched(bq_client=bq)\n",
    "print(df.shape, \"rows x cols\")\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78fac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print columns for debugging after preprocessing\n",
    "# Ensure df is defined before preprocessing!\n",
    "try:\n",
    "    df\n",
    "except NameError:\n",
    "    # If df is not defined, load it first\n",
    "    fqtn = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n",
    "    print(\"Reading from:\", fqtn)\n",
    "    bq = bigquery_connector.BigQueryConnector(project_id=PROJECT_ID)\n",
    "    df = au.fetch_purchase_data_enriched(bq_client=bq)\n",
    "    print(df.shape, \"rows x cols\")\n",
    "    df.head(3)\n",
    "\n",
    "df = au.preprocess_detailed_data(df)\n",
    "print(\"Columns in DataFrame after preprocessing:\", list(df.columns))\n",
    "\n",
    "# Ensure key columns exist with consistent names\n",
    "def _resolve(df, candidates):\n",
    "    # mirror analysis_utils._resolve_col\n",
    "    def norm(s): return re.sub(r\"[^a-z0-9]\", \"\", s.lower())\n",
    "    by_norm = {norm(c): c for c in df.columns}\n",
    "    for c in candidates:\n",
    "        if c in df.columns: return c\n",
    "        if norm(c) in by_norm: return by_norm[norm(c)]\n",
    "    return candidates[0]  # fallback to first candidate, but user must check resolved value\n",
    "\n",
    "# Force correct column names for robustness\n",
    "COL_CLASS3 = \"Class3\"\n",
    "COL_PNUM   = _resolve(df, [\"ProductNumber\"])\n",
    "COL_PDESC  = _resolve(df, [\"ProductDescription\"])\n",
    "COL_EUR    = _resolve(df, [\"Amount Eur\", \"Purchase Amount Eur\", \"amount_eur\", \"purchase_amount_eur\"])\n",
    "COL_QTY    = _resolve(df, [\"Purchase Quantity\", \"Quantity\"])\n",
    "print(\"Resolved:\", {\"Class3\":COL_CLASS3, \"ProductNumber\":COL_PNUM, \"ProductDescription\":COL_PDESC, \"â‚¬\":COL_EUR, \"Qty\":COL_QTY})\n",
    "\n",
    "# Normalize spend to numeric\n",
    "df[COL_EUR] = pd.to_numeric(df[COL_EUR], errors=\"coerce\").fillna(0.0)\n",
    "df[COL_QTY] = pd.to_numeric(df[COL_QTY], errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78128afd",
   "metadata": {},
   "source": [
    "### ABCDE Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311d236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.data_processing.analysis_utils import compute_abcde_per_class3\n",
    "\n",
    "df_seg = compute_abcde_per_class3(\n",
    "    df,\n",
    "    COL_CLASS3,   # class_col\n",
    "    COL_PNUM,     # product_col\n",
    "    COL_EUR,      # spend_col\n",
    "    qty_col=COL_QTY,\n",
    "    tiers=[\"A\",\"B\",\"C\",\"D\",\"E\"],\n",
    "    tier_thresholds=[0.8,0.95,0.99,0.999],\n",
    "    min_products_per_tier=5,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1389a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run segmentation and merge results\n",
    "tags = compute_abcde_per_class3(df, COL_CLASS3, COL_PNUM, COL_EUR)\n",
    "df_seg = df.merge(tags, on=[COL_CLASS3, COL_PNUM], how=\"left\")\n",
    "df_seg.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c911d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize tiers by Class3 (robust, no EUR mismatch, dedup tags)\n",
    "# 1. Aggregate EUR per product in original df\n",
    "# 2. Merge segmentation tags (deduplicated) onto this per-product EUR DataFrame\n",
    "# 3. Use this for summary\n",
    "\n",
    "# Aggregate EUR per product\n",
    "per_product = df.groupby([COL_CLASS3, COL_PNUM], as_index=False)[COL_EUR].sum()\n",
    "# Compute and deduplicate segmentation tags\n",
    "seg_tags = compute_abcde_per_class3(df, COL_CLASS3, COL_PNUM, COL_EUR)\n",
    "seg_tags = seg_tags.drop_duplicates(subset=[COL_CLASS3, COL_PNUM])\n",
    "# Merge segmentation tags\n",
    "per_product = per_product.merge(\n",
    "    seg_tags,\n",
    "    on=[COL_CLASS3, COL_PNUM], how=\"left\"\n",
    ")\n",
    "\n",
    "# Print columns for debugging\n",
    "print(\"Columns in per_product:\", list(per_product.columns))\n",
    "\n",
    "# Find EUR column by substring match (case-insensitive)\n",
    "eur_cols = [col for col in per_product.columns if 'eur' in col.lower()]\n",
    "print(\"Columns containing 'eur':\", eur_cols)\n",
    "if eur_cols:\n",
    "    actual_eur_col = eur_cols[0]\n",
    "    print(f\"Using EUR column for aggregation: {actual_eur_col}\")\n",
    "else:\n",
    "    raise KeyError(f\"No column containing 'eur' found in per_product. Columns: {list(per_product.columns)}\")\n",
    "\n",
    "# Summary by Class3 and Segmentation\n",
    "summary = (\n",
    "    per_product.groupby([COL_CLASS3, \"Segmentation\"], as_index=False)\n",
    "    .agg(\n",
    "        products=(COL_PNUM, \"nunique\"),\n",
    "        purchase_amount_eur_total=(actual_eur_col, \"sum\")\n",
    "    )\n",
    "    .sort_values([COL_CLASS3, \"Segmentation\"])\n",
    ")\n",
    "# Format # products as e.g. 1,000\n",
    "summary[\"products_fmt\"] = summary[\"products\"].apply(lambda x: f\"{x:,}\")\n",
    "# Format purchase_amount_eur_total as e.g. 1,000,000 EUR\n",
    "summary[\"purchase_amount_eur_total_fmt\"] = summary[\"purchase_amount_eur_total\"].apply(lambda x: f\"{x:,.0f} EUR\")\n",
    "summary[[COL_CLASS3, \"Segmentation\", \"products_fmt\", \"purchase_amount_eur_total_fmt\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71989d29",
   "metadata": {},
   "source": [
    "### Data validation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce899f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the sum of purchase_amount_eur_total in summary matches the raw total from the original table\n",
    "total_summary = summary[\"purchase_amount_eur_total\"].sum()\n",
    "print(f\"Summary tiers by Class3 total: {total_summary:,.0f} EUR\")\n",
    "if np.isclose(total_summary, raw_total):\n",
    "    print(\"Summary tiers by Class3 total matches raw total from original table.\")\n",
    "else:\n",
    "    print(\"Summary tiers by Class3 total does NOT match raw total from original table!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2d41d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the raw total from the original table (no deduplication)\n",
    "raw_total = df[COL_EUR].sum()\n",
    "print(f\"Raw total from original table: {raw_total:,.0f} EUR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c42d30",
   "metadata": {},
   "source": [
    "### Export results to .xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc44d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print available columns in df_seg for debugging\n",
    "print(\"Available columns in df_seg:\", list(df_seg.columns))\n",
    "\n",
    "# Try to select only columns that exist\n",
    "export_cols = [\n",
    "    COL_PNUM, COL_PDESC, COL_CLASS3, \"Segmentation\",\n",
    "    \"salesRounding_x\", \"year_authorization_x\", COL_EUR + \"_x\",\n",
    "    \"head_shape_x\",\"thread_type_x\",\"head_height_x\",\"head_outside_diameter_width_x\",\"quality_x\",\n",
    "    \"surface_treatment_x\",\"material_x\",\"din_standard_x\",\"thread_diameter_x\",\"length_x\",\"height_x\",\n",
    "    \"total_height_x\",\"width_x\",\"iso_standard_x\",\"inside_diameter_x\",\"outside_diameter_x\",\n",
    "    \"thickness_x\",\"designed_for_thread_x\",\"total_length_x\",\"head_type_x\",\"thread_length_x\"\n",
    "]\n",
    "existing_export_cols = [col for col in export_cols if col in df_seg.columns]\n",
    "\n",
    "out_dir = Path(OUTPUT_DIR); out_dir.mkdir(exist_ok=True, parents=True)\n",
    "written = xpu.export_year_split_purchase_quantity(\n",
    "    bq,\n",
    "    output_dir=str(out_dir),\n",
    "    table=fqtn,                      # use enriched table\n",
    "    fmt_thousands=True,\n",
    "    merged_header_label=\"PurchaseQuantity\",\n",
    "    segmentation_df=df_seg[existing_export_cols].drop_duplicates(subset=[COL_PNUM], keep=\"first\"),\n",
    "    segmentation_col=\"Segmentation\"\n",
    ")\n",
    "print(\"Files written:\", len(written))\n",
    "for p in written[:10]:\n",
    "    print(\" -\", p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
