{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d0bb1e1",
   "metadata": {},
   "source": [
    "# Part 1: Spend analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3371c38",
   "metadata": {},
   "source": [
    "### ðŸ“¦ Cell 1 â€” Imports & prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c526606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silence warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "from source.db_connect.bigquery_connector import BigQueryConnector\n",
    "from source.data_processing.analysis_utils import (\n",
    "    preprocess_detailed_data,\n",
    "    compute_abc_tiers,\n",
    "    compute_kmeans_tiers,\n",
    "    fetch_purchase_data,\n",
    ")\n",
    "\n",
    "# Helper for case-insensitive column resolution\n",
    "def _resolve_column(df, target_name, candidates, synth_func=None, warn_msg=None):\n",
    "    lower_map = {c.lower(): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        key = cand.lower()\n",
    "        if key in lower_map:\n",
    "            original = lower_map[key]\n",
    "            if original != target_name:\n",
    "                df = df.rename(columns={original: target_name})\n",
    "            return df, True\n",
    "    # Synthesize if requested\n",
    "    if synth_func is not None and target_name not in df.columns:\n",
    "        df[target_name] = synth_func(df)\n",
    "        if warn_msg:\n",
    "            print(warn_msg)\n",
    "        return df, True\n",
    "    return df, False\n",
    "\n",
    "# Load data\n",
    "bq = BigQueryConnector()\n",
    "df_raw = fetch_purchase_data(bq)\n",
    "\n",
    "# Preprocess (normalizes names / values)\n",
    "df = preprocess_detailed_data(df_raw)\n",
    "\n",
    "# Spend column resolution\n",
    "SPEND_COL = \"total_spend\"\n",
    "if SPEND_COL not in df.columns:\n",
    "    spend_candidates = [\n",
    "        \"total_spend\",\"purchase_amount_eur\",\"amount_eur\",\"Amount Eur\",\"total_purchase_amount_eur\"\n",
    "    ]\n",
    "    candidate = next((c for c in spend_candidates if c in df.columns), None)\n",
    "    if candidate:\n",
    "        df = df.copy()\n",
    "        df[SPEND_COL] = pd.to_numeric(df[candidate], errors=\"coerce\").fillna(0)\n",
    "    elif {\"purchase_amount_eur\",\"purchase_quantity\"} <= set(df.columns):\n",
    "        df = df.copy()\n",
    "        df[SPEND_COL] = (\n",
    "            pd.to_numeric(df[\"purchase_amount_eur\"], errors=\"coerce\").fillna(0) *\n",
    "            pd.to_numeric(df[\"purchase_quantity\"], errors=\"coerce\").fillna(0)\n",
    "        )\n",
    "    elif {\"Amount Eur\",\"Quantity\"} <= set(df.columns):\n",
    "        df = df.copy()\n",
    "        df[SPEND_COL] = pd.to_numeric(df[\"Amount Eur\"], errors=\"coerce\").fillna(0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unable to derive spend column. Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Resolve / construct Class3 when explicit Class3 missing\n",
    "class3_candidates = [\n",
    "    \"Class3\",\"class3\",\"class_3\",\"Class3Description\",\"class3description\",\"Class3Number\",\"class3number\"\n",
    "]\n",
    "class3_col = next((c for c in class3_candidates if c in df.columns), None)\n",
    "if not class3_col:\n",
    "    generic_class_cols = [c for c in df.columns if c.lower() == \"class\"]\n",
    "    if generic_class_cols:\n",
    "        if len(generic_class_cols) > 1:\n",
    "            new_cols = list(df.columns)\n",
    "            seen = {}\n",
    "            for i, col in enumerate(new_cols):\n",
    "                if col.lower() == 'class':\n",
    "                    seen[col] = seen.get(col, 0) + 1\n",
    "                    if seen[col] > 1:\n",
    "                        new_cols[i] = f\"Class_dup{seen[col]-1}\"  # first stays 'Class'\n",
    "            df.columns = new_cols\n",
    "            generic_class_cols = [c for c in df.columns if c.lower() in {\"class\",\"class_dup0\",\"class_dup1\",\"class_dup2\"}]\n",
    "        ordered = generic_class_cols\n",
    "        mapping = {}\n",
    "        if len(ordered) >= 1:\n",
    "            mapping[ordered[0]] = \"Class2\" if len(ordered) > 1 else \"Class3\"\n",
    "        if len(ordered) >= 2:\n",
    "            mapping[ordered[1]] = \"Class3\"\n",
    "        if len(ordered) >= 3:\n",
    "            mapping[ordered[2]] = \"Class4\"\n",
    "        df = df.rename(columns=mapping)\n",
    "        if \"Class3\" not in df.columns:\n",
    "            synth_source = next((c for c in [\"Class2\",\"Class4\"] if c in df.columns), None)\n",
    "            if synth_source:\n",
    "                df[\"Class3\"] = df[synth_source]\n",
    "            else:\n",
    "                raise KeyError(f\"Could not synthesize Class3; columns: {df.columns.tolist()}\")\n",
    "    else:\n",
    "        raise KeyError(f\"Could not locate a Class3-like column. Available columns: {df.columns.tolist()}\")\n",
    "else:\n",
    "    if class3_col != \"Class3\":\n",
    "        df = df.rename(columns={class3_col: \"Class3\"})\n",
    "\n",
    "# Resolve GroupVendor (case-insensitive)\n",
    "if \"GroupVendor\" not in df.columns:\n",
    "    vendor_candidates = [\"GroupVendor\",\"crm_main_group_vendor\",\"group_vendor\",\"Group Vendor\",\"crm_group_vendor\"]\n",
    "    lower_map = {c.lower(): c for c in df.columns}\n",
    "    for cand in vendor_candidates:\n",
    "        if cand.lower() in lower_map:\n",
    "            original = lower_map[cand.lower()]\n",
    "            if original != \"GroupVendor\":\n",
    "                df = df.rename(columns={original: \"GroupVendor\"})\n",
    "            break\n",
    "\n",
    "# Resolve ProductNumber (case-insensitive + synth)\n",
    "prod_candidates = [\n",
    "    \"ProductNumber\",\"productnumber\",\"product_number\",\"ProductNo\",\"productno\",\"ProductID\",\"productid\",\"ProductId\",\n",
    "    \"MaterialNumber\",\"materialnumber\",\"material_number\",\"Material\",\"ItemNumber\",\"itemnumber\",\"item_number\"\n",
    "]\n",
    "df, _ = _resolve_column(\n",
    "    df,\n",
    "    \"ProductNumber\",\n",
    "    prod_candidates,\n",
    "    synth_func=lambda d: \"SYN_\" + d.index.astype(str),\n",
    "    warn_msg=\"Warning: No product identifier column found (case-insensitive). Synthesized 'ProductNumber'.\"\n",
    ")\n",
    "\n",
    "# Resolve ProductDescription (case-insensitive + fallback to ProductNumber)\n",
    "desc_candidates = [\n",
    "    \"ProductDescription\",\"productdescription\",\"product_description\",\"Description\",\"Product Desc\",\"ProductDesc\",\n",
    "    \"MaterialDescription\",\"materialdescription\",\"MaterialDesc\",\"ItemDescription\",\"itemdescription\",\"ItemDesc\"\n",
    "]\n",
    "df, found_desc = _resolve_column(\n",
    "    df,\n",
    "    \"ProductDescription\",\n",
    "    desc_candidates,\n",
    "    synth_func=lambda d: d[\"ProductNumber\"].astype(str),\n",
    "    warn_msg=\"Warning: No product description column found. Using ProductNumber as placeholder ProductDescription.\"\n",
    ")\n",
    "\n",
    "# Resolve PurchaseQuantity (case-insensitive + zero synth)\n",
    "qty_candidates = [\n",
    "    \"PurchaseQuantity\",\"purchasequantity\",\"purchase_quantity\",\"Quantity\",\"Qty\",\"Purchase Qty\",\"purchase_qty\",\"qty\"\n",
    "]\n",
    "df, found_qty = _resolve_column(\n",
    "    df,\n",
    "    \"PurchaseQuantity\",\n",
    "    qty_candidates,\n",
    "    synth_func=lambda d: 0,\n",
    "    warn_msg=\"Warning: No purchase quantity column found. Created placeholder 'PurchaseQuantity'=0.\"\n",
    ")\n",
    "\n",
    "# Add ABC tiers\n",
    "df = compute_abc_tiers(df, spend_col=SPEND_COL, tier_col=\"abc_tier\")\n",
    "\n",
    "# Analysis year\n",
    "if \"year\" in df.columns:\n",
    "    YEAR = int(pd.to_numeric(df[\"year\"], errors=\"coerce\").dropna().mode().iloc[0])\n",
    "else:\n",
    "    YEAR = datetime.now().year\n",
    "\n",
    "print(f\"Rows: {len(df):,} | Cols: {len(df.columns)} | YEAR={YEAR}\")\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(f\"Sample data:\\n{df.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f53ea",
   "metadata": {},
   "source": [
    "### ðŸ“ˆ Cell 2 â€” Scatter: distribution of spend per Class3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679201e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-product spend (ensure numeric)\n",
    "work = df.copy()\n",
    "work[SPEND_COL] = pd.to_numeric(work[SPEND_COL], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# Rank within each Class3 to get a nice spread on X\n",
    "work[\"rank_in_class3\"] = work.groupby(\"Class3\")[SPEND_COL].rank(method=\"first\", ascending=False)\n",
    "\n",
    "# Determine legend (Class3) ordering by total spend descending\n",
    "class3_spend = work.groupby(\"Class3\")[SPEND_COL].sum().sort_values(ascending=False)\n",
    "class3_order = class3_spend.index.tolist()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(\n",
    "    data=work,\n",
    "    x=\"rank_in_class3\",\n",
    "    y=SPEND_COL,\n",
    "    hue=\"Class3\",\n",
    "    hue_order=class3_order,\n",
    "    alpha=0.6,\n",
    "    s=25\n",
    ")\n",
    "plt.yscale(\"log\")  # spend is typically heavy-tailed\n",
    "plt.title(\"Spend distribution per Class3 (log scale)\")\n",
    "plt.xlabel(\"Rank within Class3 (1 = highest spend)\")\n",
    "plt.ylabel(\"Spend\")\n",
    "plt.legend(title=\"Class3 (desc spend)\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d792fe",
   "metadata": {},
   "source": [
    "### ðŸ·ï¸ Cell 3 â€” Scatter: distribution of spend by Group Vendor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d905d9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "work = df.copy()\n",
    "work[SPEND_COL] = pd.to_numeric(work[SPEND_COL], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# Limit legend noise: focus on top N vendors by total spend\n",
    "N_VENDORS = 10\n",
    "top_vendors = (work.groupby(\"GroupVendor\")[SPEND_COL]\n",
    "               .sum()\n",
    "               .sort_values(ascending=False)\n",
    "               .head(N_VENDORS)\n",
    "               .index)\n",
    "work[\"VendorTop\"] = np.where(work[\"GroupVendor\"].isin(top_vendors), work[\"GroupVendor\"], \"Other\")\n",
    "\n",
    "# Rank globally by spend\n",
    "work[\"rank_global\"] = work[SPEND_COL].rank(method=\"first\", ascending=False)\n",
    "\n",
    "# Legend ordering: VendorTop descending by total spend, keeping 'Other' last if present\n",
    "vendor_spend = work.groupby(\"VendorTop\")[SPEND_COL].sum().sort_values(ascending=False)\n",
    "vendor_order = [v for v in vendor_spend.index if v != \"Other\"] + ([\"Other\"] if \"Other\" in vendor_spend.index else [])\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(\n",
    "    data=work,\n",
    "    x=\"rank_global\",\n",
    "    y=SPEND_COL,\n",
    "    hue=\"VendorTop\",\n",
    "    hue_order=vendor_order,\n",
    "    alpha=0.6,\n",
    "    s=25\n",
    ")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(f\"Spend distribution by Group Vendor (Top {N_VENDORS})\")\n",
    "plt.xlabel(\"Global rank (1 = highest spend)\")\n",
    "plt.ylabel(\"Spend\")\n",
    "plt.legend(title=\"GroupVendor (desc spend)\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19349109",
   "metadata": {},
   "source": [
    "### ðŸ§® Cell 4 â€” Count of products by spend interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b9c6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "work = df.copy()\n",
    "work[SPEND_COL] = pd.to_numeric(work[SPEND_COL], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# Use log-spaced bins for heavy-tailed spend\n",
    "min_pos = max(work[SPEND_COL].replace(0, np.nan).min(), 1e-6)\n",
    "max_val = max(work[SPEND_COL].max(), 1)\n",
    "BINS = 15\n",
    "bins = np.geomspace(min_pos, max_val, BINS)\n",
    "\n",
    "work[\"spend_bin\"] = pd.cut(work[SPEND_COL].clip(lower=min_pos), bins=bins, include_lowest=True)\n",
    "counts = work[\"spend_bin\"].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "counts.plot(kind=\"bar\")\n",
    "plt.title(\"Count of products by spend interval (log-spaced bins)\")\n",
    "plt.xlabel(\"Spend interval (EUR)\")\n",
    "plt.ylabel(\"Count of products\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (Optional) show a small table\n",
    "display(counts.to_frame(\"count\").head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703672fe",
   "metadata": {},
   "source": [
    "### ðŸ¤– Cell 5 â€” Cluster products at Class3 level by purchase amount (EUR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977849f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster per Class3 on *purchase amount EUR* at product level\n",
    "# If your SPEND_COL already equals purchase_amount_eur over the window, we can reuse SPEND_COL.\n",
    "# Otherwise, aggregate directly from purchase_amount_eur.\n",
    "\n",
    "if \"purchase_amount_eur\" in df.columns:\n",
    "    base = (df.groupby([\"Class3\", \"ProductNumber\"], as_index=False)[\"purchase_amount_eur\"]\n",
    "              .sum()\n",
    "              .rename(columns={\"purchase_amount_eur\": \"amount_eur\"}))\n",
    "else:\n",
    "    base = (df.groupby([\"Class3\", \"ProductNumber\"], as_index=False)[SPEND_COL]\n",
    "              .sum()\n",
    "              .rename(columns={SPEND_COL: \"amount_eur\"}))\n",
    "\n",
    "# Fit k-means within each Class3 (use 3 clusters by default)\n",
    "k = 3\n",
    "out_frames = []\n",
    "for c3, g in base.groupby(\"Class3\", as_index=False):\n",
    "    g = g.copy()\n",
    "    # Clean numeric column\n",
    "    g[\"amount_eur\"] = pd.to_numeric(g[\"amount_eur\"], errors=\"coerce\")\n",
    "    # Drop NaNs / non-positive entries before clustering\n",
    "    g = g[g[\"amount_eur\"].notna() & (g[\"amount_eur\"] > 0)]\n",
    "    if g.empty:\n",
    "        continue  # nothing to cluster\n",
    "    # If fewer rows than k, assign all to tier A and continue\n",
    "    if len(g) < k:\n",
    "        g[\"kmeans_tier\"] = \"A\"\n",
    "        out_frames.append(g)\n",
    "        continue\n",
    "    # Reuse analysis_utils' kmeans on a single column by mapping to expected signature\n",
    "    g[\"total_spend\"] = g[\"amount_eur\"]\n",
    "    try:\n",
    "        g = compute_kmeans_tiers(g, spend_col=\"total_spend\", k=k, log_transform=True, tier_col=\"kmeans_tier\")\n",
    "    except Exception:\n",
    "        # fallback: simple quantile cut if sklearn missing or convergence issue\n",
    "        q = g[\"amount_eur\"].rank(pct=True)\n",
    "        g[\"kmeans_tier\"] = np.where(q <= 1/3, \"A\", np.where(q <= 2/3, \"B\", \"C\"))\n",
    "    out_frames.append(g.drop(columns=[\"total_spend\"], errors=\"ignore\"))\n",
    "\n",
    "if out_frames:\n",
    "    df_class3_clusters = pd.concat(out_frames, ignore_index=True)\n",
    "    print(df_class3_clusters.head())\n",
    "else:\n",
    "    print(\"No clusters produced (all Class3 groups empty after filtering).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1882705",
   "metadata": {},
   "source": [
    "### ðŸ“¤ Cell 6 â€” Export per Class3 to .xlsx with normalized columns and a top metadata row for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0160230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "REQUIRED_COLS = [\"ProductNumber\", \"ProductDescription\", \"PurchaseQuantity\"]\n",
    "missing = [c for c in REQUIRED_COLS if c not in df.columns]\n",
    "if missing:\n",
    "    if \"ProductDescription\" in missing:\n",
    "        desc_candidates = [\n",
    "            \"ProductDescription\",\"productdescription\",\"product_description\",\"Description\",\"Product Desc\",\"ProductDesc\",\n",
    "            \"MaterialDescription\",\"materialdescription\",\"MaterialDesc\",\"ItemDescription\",\"itemdescription\",\"ItemDesc\"\n",
    "        ]\n",
    "        desc_col = next((c for c in desc_candidates if c in df.columns), None)\n",
    "        if desc_col:\n",
    "            df = df.rename(columns={desc_col: \"ProductDescription\"})\n",
    "        else:\n",
    "            df[\"ProductDescription\"] = df.get(\"ProductNumber\", \"\").astype(str)\n",
    "            print(\"Fallback: Synthesized ProductDescription from ProductNumber.\")\n",
    "    if \"PurchaseQuantity\" in missing:\n",
    "        qty_candidates = [\n",
    "            \"PurchaseQuantity\",\"purchasequantity\",\"purchase_quantity\",\"Quantity\",\"Qty\",\"Purchase Qty\",\"purchase_qty\",\"qty\"\n",
    "        ]\n",
    "        qty_col = next((c for c in qty_candidates if c in df.columns), None)\n",
    "        if qty_col:\n",
    "            df = df.rename(columns={qty_col: \"PurchaseQuantity\"})\n",
    "        else:\n",
    "            df[\"PurchaseQuantity\"] = 0\n",
    "            print(\"Fallback: Created placeholder PurchaseQuantity=0.\")\n",
    "    missing = [c for c in REQUIRED_COLS if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required columns for export after fallback attempts: {missing}\")\n",
    "\n",
    "output_dir = os.path.join(os.getcwd(), \"exports_per_class3\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "invalid_pattern = re.compile(r\"[^A-Za-z0-9._-]+\")\n",
    "\n",
    "for c3, g in df.groupby(\"Class3\"):\n",
    "    sub = g.loc[:, REQUIRED_COLS].copy()\n",
    "    sub[\"ProductNumber\"] = sub[\"ProductNumber\"].astype(str)\n",
    "    sub[\"ProductDescription\"] = sub[\"ProductDescription\"].astype(str)\n",
    "    sub[\"PurchaseQuantity\"] = pd.to_numeric(sub[\"PurchaseQuantity\"], errors=\"coerce\")\n",
    "\n",
    "    meta = pd.DataFrame([{ \"ProductNumber\": \"\", \"ProductDescription\": \"\", \"PurchaseQuantity\": YEAR }])\n",
    "    export_df = pd.concat([meta, sub], ignore_index=True)\n",
    "\n",
    "    raw_name = str(c3)\n",
    "    safe_c3 = invalid_pattern.sub(\"_\", raw_name).strip(\"._\") or \"Class3\"\n",
    "    if len(safe_c3) > 120:\n",
    "        safe_c3 = safe_c3[:120]\n",
    "    path = os.path.join(output_dir, f\"ABC_Segmentation_{safe_c3}.xlsx\")\n",
    "    export_df.to_excel(path, index=False)\n",
    "    print(f\"âœ… Exported {path} ({len(export_df)-1} rows + metadata row) - original label '{raw_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c6a332",
   "metadata": {},
   "source": [
    "# Part 2: Enrich exports with product details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4e5b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import importlib\n",
    "\n",
    "import source.data_processing.product_utils as product_utils\n",
    "importlib.reload(product_utils)  # ensure latest chunking / schema fixes loaded\n",
    "\n",
    "EXPORT_DIR = os.path.join(os.getcwd(), \"exports_per_class3\")\n",
    "excel_paths = sorted(\n",
    "    os.path.join(EXPORT_DIR, f)\n",
    "    for f in os.listdir(EXPORT_DIR)\n",
    "    if f.lower().endswith(\".xlsx\")\n",
    ")\n",
    "\n",
    "if not excel_paths:\n",
    "    print(\"No exported Excel files found in exports_per_class3; nothing to enrich.\")\n",
    "else:\n",
    "    exports_cache = {}\n",
    "    product_numbers = set()\n",
    "    for path in excel_paths:\n",
    "        export_df = pd.read_excel(path, dtype={\"ProductNumber\": str})\n",
    "        exports_cache[path] = export_df\n",
    "        if \"ProductNumber\" not in export_df.columns:\n",
    "            continue\n",
    "        product_series = export_df[\"ProductNumber\"].astype(str).str.strip()\n",
    "        product_series = product_series.replace({\"nan\": \"\", \"None\": \"\"})\n",
    "        product_numbers.update(product_series[product_series != \"\"].tolist())\n",
    "\n",
    "    if not product_numbers:\n",
    "        print(\"No product numbers found across exports; enrichment skipped.\")\n",
    "    else:\n",
    "        # Prefer wide mapping (pivoted) first\n",
    "        candidate_funcs = [\n",
    "            \"get_product_details_mapping\",  # wide form (one row per product)\n",
    "            \"fetch_product_details\",        # long form (detail_name/detail_value)\n",
    "            \"load_product_details\",\n",
    "            \"get_product_details\",\n",
    "            \"retrieve_product_details\",\n",
    "            \"enrich_with_product_details\",\n",
    "            \"enrich_products\",\n",
    "            \"load_product_master\",\n",
    "            \"fetch_product_master\",\n",
    "        ]\n",
    "        detail_func = None\n",
    "        for name in candidate_funcs:\n",
    "            func = getattr(product_utils, name, None)\n",
    "            if callable(func):\n",
    "                detail_func = func\n",
    "                break\n",
    "        if detail_func is None:\n",
    "            for name in dir(product_utils):\n",
    "                if \"product\" in name.lower():\n",
    "                    func = getattr(product_utils, name)\n",
    "                    if callable(func):\n",
    "                        detail_func = func\n",
    "                        break\n",
    "        if detail_func is None:\n",
    "            raise AttributeError(\"Could not locate a product detail fetcher in product_utils.py.\")\n",
    "\n",
    "        sig = inspect.signature(detail_func)\n",
    "        numbers_list = sorted(product_numbers)\n",
    "        connector_aliases = {\"connector\", \"conn\", \"bq\", \"bq_connector\", \"client\", \"source_connector\", \"bqclient\", \"bq_client\"}\n",
    "        product_aliases = {\"product_numbers\", \"products\", \"product_ids\", \"ids\", \"items\"}\n",
    "        args, kwargs = [], {}\n",
    "        provided_connector = False\n",
    "        provided_numbers = False\n",
    "        accepts_kwargs = any(p.kind == inspect.Parameter.VAR_KEYWORD for p in sig.parameters.values())\n",
    "\n",
    "        def _is_connector_name(name: str) -> bool:\n",
    "            lname = name.lower()\n",
    "            return lname in connector_aliases or any(x in lname for x in [\"bq\", \"connector\", \"client\"])\n",
    "\n",
    "        def _is_products_name(name: str) -> bool:\n",
    "            lname = name.lower()\n",
    "            return lname in product_aliases or any(x in lname for x in [\"product\", \"items\", \"ids\"])\n",
    "\n",
    "        for param in sig.parameters.values():\n",
    "            if param.kind in (inspect.Parameter.POSITIONAL_ONLY, inspect.Parameter.POSITIONAL_OR_KEYWORD):\n",
    "                if param.default is inspect._empty:  # required positional\n",
    "                    if _is_connector_name(param.name) and not provided_connector:\n",
    "                        args.append(bq)\n",
    "                        provided_connector = True\n",
    "                    elif _is_products_name(param.name) and not provided_numbers:\n",
    "                        args.append(numbers_list)\n",
    "                        provided_numbers = True\n",
    "                    else:\n",
    "                        ann = param.annotation\n",
    "                        if ann is not inspect._empty:\n",
    "                            try:\n",
    "                                from source.db_connect.bigquery_connector import BigQueryConnector as _BQC\n",
    "                                if ann is _BQC and not provided_connector:\n",
    "                                    args.append(bq)\n",
    "                                    provided_connector = True\n",
    "                                    continue\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                        raise TypeError(f\"Unsupported required parameter '{param.name}' in {detail_func.__name__}.\")\n",
    "                else:  # optional keyword\n",
    "                    if _is_connector_name(param.name) and not provided_connector:\n",
    "                        kwargs[param.name] = bq\n",
    "                        provided_connector = True\n",
    "                    elif _is_products_name(param.name) and not provided_numbers:\n",
    "                        kwargs[param.name] = numbers_list\n",
    "                        provided_numbers = True\n",
    "            elif param.kind == inspect.Parameter.KEYWORD_ONLY and param.default is inspect._empty:\n",
    "                if _is_connector_name(param.name) and not provided_connector:\n",
    "                    kwargs[param.name] = bq\n",
    "                    provided_connector = True\n",
    "                elif _is_products_name(param.name) and not provided_numbers:\n",
    "                    kwargs[param.name] = numbers_list\n",
    "                    provided_numbers = True\n",
    "\n",
    "        if not provided_numbers:\n",
    "            if any(_is_products_name(p.name) for p in sig.parameters.values()):\n",
    "                target = next(p.name for p in sig.parameters.values() if _is_products_name(p.name))\n",
    "                kwargs[target] = numbers_list\n",
    "                provided_numbers = True\n",
    "            elif accepts_kwargs:\n",
    "                kwargs[\"product_numbers\"] = numbers_list\n",
    "                provided_numbers = True\n",
    "        if not provided_numbers:\n",
    "            raise TypeError(f\"{detail_func.__name__} must accept product numbers as an argument.\")\n",
    "\n",
    "        if not provided_connector:\n",
    "            for p in sig.parameters.values():\n",
    "                if _is_connector_name(p.name):\n",
    "                    kwargs[p.name] = bq\n",
    "                    provided_connector = True\n",
    "                    break\n",
    "        if not provided_connector:\n",
    "            for p in sig.parameters.values():\n",
    "                ann = p.annotation\n",
    "                if ann is not inspect._empty:\n",
    "                    try:\n",
    "                        from source.db_connect.bigquery_connector import BigQueryConnector as _BQC\n",
    "                        if ann is _BQC:\n",
    "                            kwargs[p.name] = bq\n",
    "                            provided_connector = True\n",
    "                            break\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "        try:\n",
    "            detail_df = detail_func(*args, **kwargs)\n",
    "        except Exception as exc:\n",
    "            raise RuntimeError(f\"Failed to fetch product details via {detail_func.__name__}: {exc}\") from exc\n",
    "\n",
    "        if detail_df is None:\n",
    "            raise ValueError(f\"{detail_func.__name__} returned None; cannot enrich exports.\")\n",
    "        if not isinstance(detail_df, pd.DataFrame):\n",
    "            detail_df = pd.DataFrame(detail_df)\n",
    "        if detail_df.empty:\n",
    "            print(f\"{detail_func.__name__} returned no product details; enrichment skipped.\")\n",
    "        else:\n",
    "            details = detail_df.copy()\n",
    "            if details.columns.duplicated().any():\n",
    "                details = details.loc[:, ~details.columns.duplicated()]\n",
    "            renames = {}\n",
    "            for col in details.columns:\n",
    "                low = col.lower()\n",
    "                if low in {\"productnumber\", \"product_number\", \"product_id\", \"productid\"}:\n",
    "                    renames[col] = \"ProductNumber\"\n",
    "            if renames:\n",
    "                details = details.rename(columns=renames)\n",
    "            if \"ProductNumber\" not in details.columns:\n",
    "                raise KeyError(\"Product details result must include a 'ProductNumber' column.\")\n",
    "            details[\"ProductNumber\"] = details[\"ProductNumber\"].astype(str).str.strip()\n",
    "            details[\"ProductNumber\"] = details[\"ProductNumber\"].replace({\"nan\": \"\", \"None\": \"\"})\n",
    "            details = details[details[\"ProductNumber\"] != \"\"]\n",
    "            if details.empty:\n",
    "                print(\"Product details dataframe contains no usable ProductNumber values; skipping enrichment.\")\n",
    "            else:\n",
    "                # Wide vs long handling: if detail_name present, pivot; else assume already wide.\n",
    "                if {\"detail_name\", \"detail_value\"} <= set(details.columns):\n",
    "                    # pivot to wide form for merging new attribute columns\n",
    "                    details = details.pivot_table(\n",
    "                        index=\"ProductNumber\",\n",
    "                        columns=\"detail_name\",\n",
    "                        values=\"detail_value\",\n",
    "                        aggfunc=\"first\"\n",
    "                    ).reset_index()\n",
    "                    details.columns.name = None\n",
    "                    details.columns = [str(c) for c in details.columns]\n",
    "\n",
    "                detail_cols = [c for c in details.columns if c != \"ProductNumber\"]\n",
    "                enriched_files = 0\n",
    "\n",
    "                for path, export_df in exports_cache.items():\n",
    "                    if \"ProductNumber\" not in export_df.columns:\n",
    "                        continue\n",
    "                    prod_series = export_df[\"ProductNumber\"].astype(str).str.strip()\n",
    "                    prod_series = prod_series.replace({\"nan\": \"\", \"None\": \"\"})\n",
    "                    meta_mask = prod_series == \"\"\n",
    "                    meta_rows = export_df.loc[meta_mask].copy()\n",
    "                    data_rows = export_df.loc[~meta_mask].copy()\n",
    "                    if data_rows.empty:\n",
    "                        continue\n",
    "                    data_rows[\"ProductNumber\"] = prod_series[~meta_mask]\n",
    "\n",
    "                    merged = data_rows.merge(details, on=\"ProductNumber\", how=\"left\", suffixes=(\"\", \"_detail\"))\n",
    "                    for col in detail_cols:\n",
    "                        detail_col = f\"{col}_detail\"\n",
    "                        if col in merged.columns and detail_col in merged.columns:\n",
    "                            merged[col] = merged[col].where(merged[col].notna(), merged[detail_col])\n",
    "                            merged.drop(columns=detail_col, inplace=True)\n",
    "                        elif detail_col in merged.columns:\n",
    "                            merged.rename(columns={detail_col: col}, inplace=True)\n",
    "\n",
    "                    original_cols = list(export_df.columns)\n",
    "                    new_cols = [c for c in detail_cols if c not in original_cols]\n",
    "                    final_cols = original_cols + new_cols\n",
    "\n",
    "                    merged = merged.reindex(columns=final_cols)\n",
    "                    if not meta_rows.empty:\n",
    "                        meta_rows = meta_rows.reindex(columns=final_cols)\n",
    "                        updated = pd.concat([meta_rows, merged], ignore_index=True)\n",
    "                    else:\n",
    "                        updated = merged\n",
    "\n",
    "                    updated.to_excel(path, index=False)\n",
    "                    enriched_files += 1\n",
    "                    print(f\"âœ… Enriched {os.path.basename(path)} ({len(new_cols)} new columns).\")\n",
    "\n",
    "                if enriched_files == 0:\n",
    "                    print(\"No Excel files were updated; no matching product numbers found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
